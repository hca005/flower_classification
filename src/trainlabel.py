# -*- coding: utf-8 -*-
"""trainlabel

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KmaF4BPUoBp0DerW_eUxQtjsGzITQDym
"""

import argparse
import time
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder

from model_vit import get_model
from transforms import get_transforms

def accuracy(logits, y):
    preds = logits.argmax(dim=1)
    return (preds == y).float().mean().item()

def train_one_epoch(model, loader, device, optimizer, criterion):
    model.train()
    total_loss, total_acc, n = 0.0, 0.0, 0
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        logits = model(x)
        loss = criterion(logits, y)
        loss.backward()
        optimizer.step()

        bs = x.size(0)
        total_loss += loss.item() * bs
        total_acc += accuracy(logits, y) * bs
        n += bs
    return total_loss / n, total_acc / n

@torch.no_grad()
def evaluate(model, loader, device, criterion):
    model.eval()
    total_loss, total_acc, n = 0.0, 0.0, 0
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        bs = x.size(0)
        total_loss += loss.item() * bs
        total_acc += accuracy(logits, y) * bs
        n += bs
    return total_loss / n, total_acc / n

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_dir", type=str, default="data")
    ap.add_argument("--epochs", type=int, default=5)
    ap.add_argument("--batch_size", type=int, default=16)
    ap.add_argument("--lr", type=float, default=3e-4)
    ap.add_argument("--img_size", type=int, default=224)
    ap.add_argument("--ls", type=float, default=0.1)  # ✅ label smoothing factor
    args = ap.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"

    train_tf, eval_tf = get_transforms(img_size=args.img_size, augment=False)  # ❌ no aug here
    train_ds = ImageFolder(Path(args.data_dir) / "train", transform=train_tf)
    val_ds   = ImageFolder(Path(args.data_dir) / "val",   transform=eval_tf)

    num_classes = len(train_ds.classes)

    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=0)
    val_loader   = DataLoader(val_ds,   batch_size=args.batch_size, shuffle=False, num_workers=0)

    model = get_model(num_classes).to(device)

    # ✅ LABEL SMOOTHING HERE
    criterion = nn.CrossEntropyLoss(label_smoothing=args.ls)
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)

    out_dir = Path("outputs") / "vit_label"
    out_dir.mkdir(parents=True, exist_ok=True)
    best_path = out_dir / "best.pt"
    best_val_acc = -1.0

    for epoch in range(1, args.epochs + 1):
        t0 = time.time()
        tr_loss, tr_acc = train_one_epoch(model, train_loader, device, optimizer, criterion)
        va_loss, va_acc = evaluate(model, val_loader, device, criterion)
        sec = time.time() - t0

        print(f"[LS={args.ls}] epoch {epoch}/{args.epochs} | "
              f"train loss {tr_loss:.4f} acc {tr_acc:.4f} | "
              f"val loss {va_loss:.4f} acc {va_acc:.4f} | {sec:.1f}s")

        if va_acc > best_val_acc:
            best_val_acc = va_acc
            torch.save(
                {"state_dict": model.state_dict(),
                 "num_classes": num_classes,
                 "classes": train_ds.classes,
                 "img_size": args.img_size,
                 "label_smoothing": args.ls},
                best_path
            )
            print(f"✅ Saved best to {best_path} (val_acc={best_val_acc:.4f})")

if __name__ == "__main__":
    main()